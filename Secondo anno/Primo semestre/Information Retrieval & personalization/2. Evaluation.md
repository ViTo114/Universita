# INTRODUZIONE ALLA VALUTAZIONE IN IR
La valutazione è una fase cruciale nel ciclo di vita di qualsiasi sistema di Information Retrieval (IR), come un motore di ricerca. Senza un metodo per misurare oggettivamente la qualità dei risultati forniti, sarebbe impossibile migliorare il sistema in modo sistematico e affidabile. La valutazione ci permette di quantificare le performance, confrontare diverse versioni di un algoritmo e comprendere se le modifiche apportate producono un reale beneficio per l'utente.

Quando si parla di "qualità" di un motore di ricerca, si fa riferimento a tre dimensioni principali:

1. **Efficacia (Effectiveness/quality of result lists):** Questa è spesso la dimensione più importante dal punto di vista dell'utente finale. Riguarda la capacità del sistema di restituire documenti pertinenti per una data query, ordinandoli in modo che i più rilevanti appaiano nelle prime posizioni. L'efficacia misura, in sostanza, quanto bene il sistema soddisfa il bisogno informativo dell'utente.

2. **Efficienza (Efficiency):** Questo aspetto si concentra sulle risorse computazionali necessarie al sistema per funzionare. Include metriche come il tempo di risposta (quanto velocemente vengono mostrati i risultati) e il carico di lavoro sul server. Un sistema efficace ma estremamente lento può risultare frustrante e poco pratico.

3. **Usabilità (Usability):** L'usabilità si riferisce alla facilità e alla piacevolezza con cui un utente può interagire con l'interfaccia del motore di ricerca. Include la chiarezza della presentazione dei risultati, la facilità di formulare query e l'esperienza utente complessiva.


Per raccogliere i dati necessari alla valutazione, esistono due approcci principali. Il primo consiste nell'interrogare direttamente gli utenti, ad esempio tramite test in laboratorio o questionari. Il secondo, più scalabile e riproducibile, prevede l'utilizzo di un **"test set"** statico. Un test set è una collezione di risorse standardizzate che include tipicamente:

• **Compiti di ricerca e bisogni informativi**, che descrivono scenari utente realistici.

• Un insieme di **query** rappresentative di tali bisogni.

• **Giudizi di rilevanza (relevance judgments)**, ovvero delle annotazioni, tipicamente manuali, che specificano quali documenti nella collezione sono da considerarsi pertinenti per ciascuna query.


Per garantire che queste valutazioni siano rigorose, replicabili e confrontabili tra diversi sistemi e team di ricerca, è necessario un framework standardizzato. Storicamente, questo ruolo è stato ricoperto dal paradigma di Cranfield.




# IL PARADIGMA DI CRANFIELD: UN APPROCCIO STANDARDIZZATO
Il paradigma di Cranfield, sviluppato negli anni '60, rappresenta la metodologia fondamentale per la valutazione "in laboratorio" dei sistemi di Information Retrieval. Il suo obiettivo primario è fornire una strategia che permetta di condurre esperimenti controllati, i cui risultati siano replicabili da altri ricercatori e possano essere usati per confrontare in modo equo l'efficacia di diversi sistemi.

L'idea centrale del paradigma è la creazione e l'utilizzo di **test collection riutilizzabili**. Una test collection è un insieme di risorse standard che permette di simulare il processo di ricerca in un ambiente controllato. Secondo il paradigma di Cranfield, essa è composta da tre elementi essenziali:

• **Una collezione di documenti:** Un corpus di "retrieval units" (unità di recupero) come pagine web, email, articoli scientifici o tweet.

• **Un insieme di query (o "topic"):** Una lista di interrogazioni che simulano i bisogni informativi reali degli utenti.

• **Giudizi di rilevanza (relevance judgments):** Un elenco, per ogni query, dei documenti nella collezione che sono stati giudicati pertinenti da esperti umani.


Il seguente schema illustra come una test collection viene utilizzata per confrontare due sistemi, System A e System B.

Data una query, ad esempio `Q1`, e la stessa collezione di documenti, entrambi i sistemi vengono interrogati. System A produce la lista di risultati `RA`, mentre System B produce la lista `RB`. Per determinare quale sistema sia migliore, utilizziamo i giudizi di rilevanza preesistenti. Per `Q1`, i documenti rilevanti sono `D1`, `D2`, e `D5`. Osservando le liste, notiamo che System A ha trovato due documenti rilevanti (`D2`, `D1`), mentre System B ne ha trovati tre (`D1`, `D5`, `D2`).

L'analisi iniziale mostra che il Sistema B è superiore per la query Q1. Ma come possiamo generalizzare questo risultato e, soprattutto, quantificare questa differenza? E se il Sistema A avesse trovato i suoi due documenti rilevanti nelle prime due posizioni, mentre i tre del Sistema B si trovassero nelle posizioni 5, 8 e 10? Per catturare queste sfumature di quantità e posizione, e per calcolare una media delle performance su centinaia di query, abbiamo bisogno di un toolkit formale di metriche di valutazione.





# METRICHE BASATE SU PRECISION, RECALL E F-MEASURE
Le metriche di valutazione possono essere classificate in due categorie principali: metriche che considerano l'ordinamento dei risultati ("ranking metrics") e metriche basate su insiemi ("set metrics"). Queste ultime valutano la qualità di un sottoinsieme di risultati, tipicamente i primi _k_ (es. i primi 10), trattandoli come un insieme non ordinato. L'idea è che l'utente si concentri solo sulla prima pagina dei risultati, senza dare troppa importanza alla loro posizione esatta all'interno di quella pagina.


Le due metriche fondamentali di questa categoria sono **Precision@k** e **Recall@k**. Per comprenderle, possiamo pensare ai risultati in termini di una "tabella di confusione":

• **True Positives (tp):** Documenti rilevanti che sono stati recuperati.

• **False Positives (fp):** Documenti non rilevanti che sono stati recuperati.

• **False Negatives (fn):** Documenti rilevanti che _non_ sono stati recuperati.


Le definizioni formali sono:

• **Precision@k (Precisione):** Misura la frazione di documenti recuperati (nei primi _k_) che sono effettivamente rilevanti. Risponde alla domanda: "Quanto è 'pulita' la lista dei risultati?".

• **Recall@k (Richiamo):** Misura la frazione di tutti i documenti rilevanti presenti nella collezione che sono stati recuperati (nei primi _k_). Risponde alla domanda: "Quanti dei documenti rilevanti che esistono sono riuscito a trovare?".


Nella pratica, esiste un **trade-off tra precisione e richiamo**: migliorare una delle due metriche spesso porta a un peggioramento dell'altra. Un sistema che cerca di massimizzare il richiamo potrebbe restituire moltissimi documenti, includendo così quasi tutti quelli rilevanti ma anche molti non rilevanti (bassa precisione).

Per combinare queste due metriche in un unico valore, si utilizza la **F-Measure**, e in particolare la **F1-score**, che è la media armonica di precisione e richiamo. La F1-score dà lo stesso peso a entrambe le metriche.

`F1 = 2 * (Precision * Recall) / (Precision + Recall)`


**Esempio di Calcolo** Supponiamo che per una data query esistano **16** documenti rilevanti in tutta la collezione. Il nostro sistema restituisce una lista dei primi 10 risultati, di cui **5** sono rilevanti.

• **Precision@10:** `5 / 10 = 0.5`

• **Recall@10:** `5 / 16 = 0.3125`

• **F1-score:** `2 * (0.5 * 0.3125) / (0.5 + 0.3125) ≈ 0.38`


Nonostante la loro utilità, Precision e Recall hanno delle limitazioni importanti. In primo luogo, calcolare il richiamo totale è spesso impossibile nel mondo reale, poiché richiederebbe di avere giudizi di rilevanza completi per l'intera collezione, un'operazione quasi sempre infattibile. Ma la limitazione più significativa è che **non tengono conto della posizione dei risultati**: un documento rilevante in posizione 1 ha lo stesso peso di uno in posizione 10. Questa lacuna rende necessarie metriche più sofisticate.





# METRICHE CHE VALUTANO L'ORDINAMENTO DEI RISULTATI
Per un utente, trovare un documento rilevante in prima posizione è molto più utile che trovarlo in cinquantesima. La valutazione dell'ordinamento ("ranking") è quindi fondamentale, perché riflette molto meglio l'esperienza utente reale rispetto alle metriche di set. Esistono diverse metriche progettate proprio per pesare maggiormente i documenti rilevanti che appaiono più in alto nella lista.


Mean Reciprocal Rank (MRR)

L'**MRR** è una metrica semplice e intuitiva, ideale per compiti in cui l'utente cerca un solo risultato corretto. Esempi tipici sono la ricerca navigazionale (trovare un sito web specifico) o le domande fattuali (es. "quando è iniziata la guerra in Ucraina?"). Si calcola in due passaggi:

1. Per una singola query, si calcola il **Reciprocal Rank (RR)**, che è l'inverso della posizione (rank) del _primo_ risultato rilevante. Ad esempio, se il primo risultato rilevante è in posizione 3, l'RR è 1/3.

2. L'**MRR** è semplicemente la media degli RR calcolati su un intero set di query.

Sebbene l'MRR sia molto usato per la sua semplicità, ha ricevuto critiche statistiche poiché tratta la differenza tra il rank 1 e 2 allo stesso modo di quella tra il rank 2 e l'infinito, rendendo la media un'operazione statisticamente discutibile su questa scala. Tuttavia, rimane una metrica pragmatica e comune.



Precision-Recall Curve e Average Precision (AP)

Un modo per visualizzare il trade-off tra precisione e richiamo lungo tutta la lista dei risultati è la **curva Precision-Recall (P-R)**. Per ogni posizione nella lista, si può calcolare una coppia di valori (precisione, richiamo). Ad esempio, per la lista `R R N R N N R N R N` (con 5 documenti rilevanti in totale), al primo risultato (R) abbiamo recuperato 1/5=20% dei documenti rilevanti con una precisione di 1/1=100%. Abbiamo quindi il punto (Recall=0.2, Precision=1.0). Al quarto risultato (R), abbiamo trovato 3 documenti rilevanti su 4, corrispondenti a 3/5=60% del totale dei rilevanti, con una precisione di 3/4=75%. Unendo questi punti si ottiene la curva.

Per rendere il confronto tra sistemi più stabile, si utilizza spesso la **precisione interpolata (interpolated precision)**. Questa viene definita come la più alta precisione osservata per qualsiasi livello di richiamo superiore o uguale a quello corrente (`p_interp(r) = max_{r'≥r} p(r')`). Questo processo trasforma la curva a "dente di sega" in una curva monotonicamente decrescente, più facile da interpretare.


Tuttavia, confrontare due sistemi le cui curve P-R si incrociano può essere difficile. Per questo motivo si è introdotta una metrica singola che riassume l'intera curva: l'**Average Precision (AP)**. L'AP calcola la media dei valori di precisione ottenuti in ogni posizione in cui viene trovato un documento rilevante.

La procedura di calcolo è la seguente: si scorre la lista dei risultati e, ogni volta che si incontra un documento rilevante, si calcola la precisione fino a quella posizione. Si sommano tutti questi valori di precisione e si divide la somma per il numero totale di documenti rilevanti per quella query.


**Esempio di calcolo dell'Average Precision (AP)** Supponiamo di avere **16** documenti rilevanti in totale per una query e il nostro sistema restituisce la seguente lista di 10 risultati (`R` = Rilevante, `N` = Non Rilevante): `R R N R R N N R N N`

1. Identifichiamo le posizioni dei documenti rilevanti: 1, 2, 4, 5, 8.

2. Calcoliamo la precisione in ciascuna di queste posizioni:

    ◦ Posizione 1 (R): 1 rilevante su 1. Precision = `1/1 = 1.0`

    ◦ Posizione 2 (R): 2 rilevanti su 2. Precision = `2/2 = 1.0`

    ◦ Posizione 4 (R): 3 rilevanti su 4. Precision = `3/4 = 0.75`

    ◦ Posizione 5 (R): 4 rilevanti su 5. Precision = `4/5 = 0.8`

    ◦ Posizione 8 (R): 5 rilevanti su 8. Precision = `5/8 = 0.625`

3. Sommiamo questi valori di precisione: `1.0 + 1.0 + 0.75 + 0.8 + 0.625 = 4.175`.

4. Dividiamo per il numero totale di documenti rilevanti (16)



Mean Average Precision (MAP)

Infine, per ottenere una metrica robusta che valuti le performance medie di un sistema su un intero set di test, si calcola il **Mean Average Precision (MAP)**. Il MAP non è altro che la media degli AP calcolati per ogni singola query. È considerata una delle metriche standard e più informative per la valutazione di liste ordinate con giudizi di rilevanza binari.

Tutte le metriche viste finora, però, si basano su un'assunzione semplificatrice: un documento è o "rilevante" o "non rilevante". Molti scenari reali, tuttavia, richiedono una granularità maggiore.




# GESTIRE LA RILEVANZA A PIU' LIVELLI: CUMULATIVE GAIN E nDCG
In molti contesti, la rilevanza non è un concetto binario. Un documento può essere "perfetto", "buono", "discreto" o "inutile". Ad esempio, per una query di ricerca video, le risposte potrebbero essere valutate su una scala come: (3) Eccellente, (2) Buona, (1) Forse buona, (0) Non rilevante. È strategicamente importante che un motore di ricerca non solo trovi documenti rilevanti, ma che dia la massima priorità a quelli "eccellenti". Per catturare queste sfumature, sono state sviluppate metriche basate su giudizi di rilevanza graduati.


Cumulative Gain (CG)

Il **Cumulative Gain (CG)** è la metrica più semplice per la rilevanza a più livelli. Si calcola semplicemente sommando i punteggi di rilevanza (es. da 0 a 3) di tutti i documenti in una lista fino a una certa posizione _k_. La sua debolezza è evidente: il CG **non tiene in alcun conto la posizione dei risultati**. Una lista con un documento "eccellente" (punteggio 3) in posizione 10 avrebbe lo stesso CG di una lista con lo stesso documento in posizione 1.



Discounted Cumulative Gain (DCG)

Il **Discounted Cumulative Gain (DCG)** migliora il CG introducendo il concetto di "sconto" (discounting). L'idea è che l'utilità di un documento diminuisce logaritmicamente con la sua posizione nella lista, riflettendo la minore probabilità che un utente lo raggiunga. La formula standard è:

`DCG@k = r_1 + Σ_{i=2 to k} (r_i / log₂(i))` dove `r_i` è il punteggio di rilevanza del documento in posizione `i`. Questo significa che il guadagno del primo risultato non viene scontato, mentre il valore di ogni risultato successivo viene penalizzato in base alla sua posizione.



Normalized Discounted Cumulative Gain (nDCG)

Il valore del DCG, però, dipende dalla scala dei punteggi e dal numero di documenti rilevanti per una query, rendendo difficile confrontare le performance tra query diverse. Per risolvere questo problema, si introduce il **Normalized Discounted Cumulative Gain (nDCG)**. L'nDCG normalizza il punteggio DCG dividendolo per il **DCG ideale (iDCG)**, ovvero il DCG che si otterrebbe se i risultati fossero ordinati in modo perfetto (tutti i documenti con punteggio 3 prima, poi quelli con punteggio 2, e così via).

`nDCG@k = DCG@k / iDCG@k` Questo processo riporta il punteggio in una scala normalizzata tra 0 e 1, rendendolo una metrica potente e confrontabile tra diverse query e sistemi.

Vediamo un esempio di calcolo basato sull'immagine sottostante. Per la lista di risultati fornita (`D1`, `D2`, `D3`...), spieghiamo il calcolo passo dopo passo:

• **Cumulative Gain (CG):** È una semplice somma. Per `D1` (gain=3), il CG è 3. Per `D2` (gain=2), il CG diventa 3+2=5. Per `D3` (gain=1), il CG diventa 3+2+1=6, e così via.

• **Discounted Cumulative Gain (DCG):** Qui applichiamo lo sconto. Per `D1` (gain=3), il DCG è 3 (nessuno sconto). Per `D2` (gain=2), il guadagno viene diviso per `log₂(2)=1`, quindi il DCG diventa 3 + (2/1) = 5. Per `D3` (gain=1), il guadagno viene diviso per `log₂(3)≈1.58`, quindi il DCG diventa 3 + (2/1) + (1/1.58) ≈ 5.63.

• **Normalized DCG (nDCG):** Il valore di DCG calcolato viene poi diviso per il DCG ideale (IdealDCG) per quella stessa query, portando il punteggio finale in un intervallo tra 0 e 1.

Dopo aver esaminato gli strumenti di misurazione (le metriche), è fondamentale considerare le risorse necessarie per applicarle: le test collection.




# LA TEST COLLECTION DI RIFERIMENTO: TREC E MS MARCO
Le test collection standardizzate sono la spina dorsale della ricerca nell'ambito dell'Information Retrieval. Fornendo un terreno di gioco comune (stessi documenti, stesse query, stessi giudizi), permettono a ricercatori di tutto il mondo di confrontare i loro approcci in modo equo, riproducibile e scientificamente rigoroso.


TREC (Text REtrieval Conference)

Organizzata per oltre 30 anni dal NIST (National Institute for Standards and Technology) degli Stati Uniti, **TREC** è la più importante e longeva iniziativa di benchmarking in IR. Il suo obiettivo è fornire una piattaforma neutrale dove i team di ricerca possono valutare i loro sistemi su compiti standardizzati. Ogni anno, TREC organizza diversi **"track"**, ovvero competizioni focalizzate su compiti specifici come la ricerca in ambito clinico, la ricerca conversazionale, il fair ranking o il deep learning. I giudizi di rilevanza raccolti durante queste competizioni vengono poi resi pubblici, arricchendo il patrimonio di risorse per la ricerca futura.



MS MARCO

**MS MARCO** (Microsoft MAchine Reading COmprehension) è una delle test collection più utilizzate per l'addestramento e la valutazione di modelli di ranking moderni, specialmente quelli basati su deep learning. Rilasciata nel 2016, è una collezione su larga scala, contenente un corpus di 8.8 milioni di passaggi di testo. Le sue query sono domande in linguaggio naturale estratte anonimamente dai log del motore di ricerca Bing, rendendole molto realistiche, spesso ambigue o mal formulate. Il set di addestramento contiene 532.800 coppie (query, passaggio rilevante) su oltre 502.900 query uniche.

Una caratteristica chiave di MS MARCO è la **scarsità dei giudizi di rilevanza ("sparse/shallow judgments")**. In media, per ogni query è presente un solo giudizio positivo (un solo passaggio di testo etichettato come rilevante). Questa caratteristica ha due conseguenze importanti:

1. **Per l'addestramento:** Poiché i modelli necessitano di esempi sia positivi che negativi, i ricercatori devono campionare casualmente esempi "negativi" dal corpus. Questi, però, non sono necessariamente irrilevanti, ma semplicemente non etichettati.

2. **Per la valutazione:** Con un solo giudizio positivo, diventa difficile distinguere le performance di modelli molto avanzati.

Data la natura delle query, che sono spesso domande con una sola risposta corretta, la metrica di valutazione ufficiale per MS MARCO è l'**MRR@10**.

Dopo aver presentato gli strumenti ideali, è importante considerare le difficoltà e le sfide che si incontrano nell'applicare questi metodi nel mondo reale.




# ASPETTI PRATICI E SFIDE NELLA VALUTAZIONE
Al di là della teoria accademica, applicare un processo di valutazione rigoroso in un contesto reale, come il motore di ricerca di un sito universitario, presenta numerose sfide pratiche.

Una delle prime decisioni da prendere è la **scelta della metrica**. In teoria, la metrica dovrebbe riflettere l'obiettivo dell'utente. In pratica, tuttavia, spesso se ne riportano diverse per garantire la comparabilità con altri lavori scientifici e per avere una visione più completa. È fondamentale essere consapevoli che ogni metrica fa delle assunzioni implicite sul comportamento dell'utente. Ad esempio, la `Precision@10` assume che:

• L'utente guarderà solo i primi 10 risultati.

• L'utente non si preoccupa dell'ordine dei risultati all'interno di questa top-10.

• L'utente non è interessato a sapere se esistono altri risultati rilevanti oltre i primi 10 (recall).

La creazione di un set di valutazione valido presenta essa stessa delle sfide significative:

• **Ottenere query e documenti rappresentativi:** È necessario un numero sufficiente di query (preferibilmente centinaia) che riflettano i bisogni informativi reali degli utenti.

• **Ottenere giudizi di rilevanza "completi":** L'ideale sarebbe avere un giudizio per ogni documento per ogni query, ma questo è infattibile su collezioni reali. Una tecnica pratica è il **"pooling"**: si raccolgono i top risultati da diversi sistemi di base, si crea un "pool" di documenti unici da questo insieme, e si fanno giudicare solo quelli, assumendo che i documenti rilevanti più importanti si trovino in questo pool.

Inoltre, la creazione manuale dei giudizi di rilevanza è costosa e complessa. L'uso del crowd-sourcing può soffrire di problemi di affidabilità, mentre coinvolgere utenti reali con bisogni informativi autentici è logisticamente difficile. Un'alternativa è derivare giudizi di rilevanza dai dati di click degli utenti, ma questi dati sono notoriamente "rumorosi": un click non è necessariamente un indicatore di rilevanza.

A causa di queste limitazioni, è chiaro che il modello da laboratorio, per quanto essenziale, non può essere l'unica forma di valutazione.




# OLTRE IL PARADIGMA DI CRANFIELD: VALUTAZIONE CON UTENTI REALI
Il paradigma di Cranfield, pur essendo il fondamento della valutazione in IR, rappresenta un'analisi offline, condotta in un ambiente di laboratorio controllato. Per comprendere appieno l'impatto di un sistema e la sua reale utilità, è indispensabile osservare come esso si comporta nell'interazione con utenti reali.

Esistono tre principali alternative alla valutazione basata su test collection statiche:

1. **A-B testing:** Questo approccio, ampiamente utilizzato nell'industria, consiste nel mostrare casualmente a diversi gruppi di utenti due versioni di un algoritmo (A e B). Si misurano poi metriche di successo (es. click-through rate, conversioni) per determinare quale versione produce risultati migliori sul campo.

2. **Studi in laboratorio con utenti:** Si osserva direttamente il comportamento degli utenti mentre interagiscono con il sistema per portare a termine dei compiti. I dati quantitativi (es. tempo impiegato, numero di click) possono essere integrati con feedback qualitativi raccolti tramite questionari e interviste.

3. **Valutazione con log data e simulazione:** Questo metodo prevede l'analisi di grandi moli di dati storici (log di query e click) per comprendere il comportamento degli utenti. Si possono anche usare questi dati per creare simulazioni che tentano di prevedere come un utente interagirebbe con una nuova versione del sistema.

In conclusione, una valutazione completa ed efficace di un sistema di Information Retrieval richiede un approccio integrato: non basta affidarsi a un'unica metodologia. È la combinazione di rigorosi test offline, basati sul paradigma di Cranfield, e di una validazione online, che coinvolge utenti reali, a fornire la visione più completa e affidabile della qualità di un sistema di ricerca.




